# Gradient-Descent-Types-implementation
Gradient descent different types implemented from scratch. The project covers the Stochastic, Mini-batch, and Batch gradient descent with different types including; Ada-Grad, RMS-Prop, Adam, NAG, and Momentum-Based Gradient descent.

The optimizer comes into the picture when it tries to lower the loss function by updating the model parameters in response to the output of the loss function. Thereby helping to reach the Global Minima with the lowest loss and most accurate output.
