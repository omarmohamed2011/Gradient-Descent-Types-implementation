# Gradient-Descent-Types-implementation
This my script for gradient descent different types implemented from scratch. The project covers the Stochastic, Mini-batch, and Batch gradient descent with different types including; Ada-Grad, RMS-Prop, Adam, NAG, and Momentum-Based Gradient descent.

The optimizer comes into the picture when it tries to lower the loss function by updating the model parameters in response to the output of the loss function. Thereby helping to reach the Global Minima with the lowest loss and most accurate output.

The notebook uses a multi-variable data in different Gradient descent optimizers implemented and with different batch sizes.
